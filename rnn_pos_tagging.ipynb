{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ByXf60YBARI0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Assignment 3 : Sequence labelling with RNNs\n",
        "In this assignement we will ask you to perform POS tagging.\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the model\n",
        "*   Evaluate your best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "**Corpora**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label.\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip \n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
        "\n",
        "**Modifications**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
        "\n",
        "**Error Analysis** (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ELUfiWmXod"
      },
      "source": [
        "## Outline\n",
        "\n",
        "This notebook provides a Keras implementation of **three RNN models** with the purpose of performing sequence labelling, in particular **POS-tagging**.\n",
        "\n",
        "The main outline of the process is the following:\n",
        "- Raw data (199 labelled documents consisting of a varying number of sentences) is split into **train**, **validation** and **test** partitions and stored in a `pandas.DataFrame` object.\n",
        "- The words in the dataframe - after some preprocessing - are substituted with their *Glove embeddings* (and with custom embeddings, when needed) and their labels (POS tags) are one-hot encoded.\n",
        "- Models have the **embeddings** as input and **tags** as output.\n",
        "- To simulate a real-world scenario, the different models are trained on the first partition and evaluated on the second one; lastly, only the best-performing model is tested on the last partition.\n",
        "Moreover, the partitions are kept independent with respect to every aspect except the word embeddings (either retrieved from GloVe or computed) and the labels encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7nOCcpf2ySa"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsmKWlqm2ySa"
      },
      "source": [
        "import os, shutil, zipfile, requests  #  file management\n",
        "import sys\n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "\n",
        "# Text pre-processing\n",
        "import re\n",
        "from functools import reduce\n",
        "\n",
        "# Neural models\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# Evaluating\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Visualization\n",
        "from tqdm import tqdm # progress bars\n",
        "from tabulate import tabulate # printing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGDw4BfZ2ySa"
      },
      "source": [
        "## File operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuDaty5K2ySa"
      },
      "source": [
        "### Download and extraction\n",
        "\n",
        "We firstly take care of downloading and extracting the dataset from GitHub, if data is not found in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sykpBIr12ySa",
        "outputId": "b4f96e5d-4649-4467-f1ee-a522508df0e5"
      },
      "source": [
        "# Download and extract data\n",
        "dataset_folder = os.path.join(os.getcwd(), 'dependency_treebank')\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    # Download data\n",
        "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    dataset_zip = 'dependency_treebank.zip'\n",
        "    dataset_zip_path = os.path.join(os.getcwd(), dataset_zip)\n",
        "    open(dataset_zip, 'wb').write(r.content)\n",
        "    if os.path.exists(dataset_zip):\n",
        "        print(\"Successful download:\", dataset_zip_path)\n",
        "    # Extract the zip archive to directory 'dependency_treebank'\n",
        "    with zipfile.ZipFile('dependency_treebank.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    if os.path.exists(dataset_folder):\n",
        "        print(\"Successful extraction:\", dataset_folder)\n",
        "else:\n",
        "    print(\"Data already downloaded\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successful download: /content/dependency_treebank.zip\n",
            "Successful extraction: /content/dependency_treebank\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MKwHX92ySb"
      },
      "source": [
        "### Files splitting\n",
        "\n",
        "Let's now create (or clean and recreate, if already present) the 'splits' folder, with 'train', 'validation' and 'test' subfolders within. This makes it easier to create the dataframe rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6--TD7i2ySb",
        "outputId": "ceaa87b2-97ad-485f-bf57-3914ac66becf"
      },
      "source": [
        "# Split dataset\n",
        "splits = {\n",
        "    0: 'train',\n",
        "    1: 'validation',\n",
        "    2: 'test'\n",
        "}\n",
        "# Create directories\n",
        "split_folder = os.path.join(os.getcwd(), 'splits')\n",
        "\n",
        "# Clean 'splits' directory, if present\n",
        "if not os.path.exists(split_folder):\n",
        "    os.makedirs(split_folder)\n",
        "print(\"Cleaning splits folder...\")\n",
        "for folder in os.listdir(split_folder):\n",
        "    for filename in folder:\n",
        "        file_path = os.path.join(split_folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "print(\"Cleaned\")\n",
        "# Create subfolders\n",
        "for split in splits.values():\n",
        "    os.makedirs(os.path.join(split_folder, split), exist_ok=True)\n",
        "# Copy files into directories according to name\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    n = int(filename[4:-3])\n",
        "    # 'Label' refers to 'splits' keys (0 for train, 1 for validation, 2 for test)\n",
        "    label = -1\n",
        "    if n < 101:\n",
        "        label = 0\n",
        "    elif n < 151:\n",
        "        label = 1\n",
        "    else:\n",
        "        label = 2\n",
        "    src = os.path.join(dataset_folder, filename)\n",
        "    dst = os.path.join(split_folder, splits[label], filename)\n",
        "    shutil.copyfile(src, dst)\n",
        "# Check distribution\n",
        "for split in splits.values():\n",
        "    folder = os.path.join(split_folder, split)\n",
        "    print(f\"Files in {split}:\", min([int(filename[4:-3]) for filename in os.listdir(folder)]), \"-\",\n",
        "                               max([int(filename[4:-3]) for filename in os.listdir(folder)]))   "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning splits folder...\n",
            "Cleaned\n",
            "Files in train: 1 - 100\n",
            "Files in validation: 101 - 150\n",
            "Files in test: 151 - 199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_zZ6qDW2ySb"
      },
      "source": [
        "### `DataFrame` creation and saving\n",
        "\n",
        "We now build a dataframe with columns `['document', 'labels', 'split']` and store it into 'Dataframes' folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETu1nS2x2ySc",
        "outputId": "b1e2fdc0-b1bf-482b-fd4e-17e5c53faf95"
      },
      "source": [
        "dataframe_folder = os.path.join(os.getcwd(), \"Dataframes\")\n",
        "if not os.path.exists(dataframe_folder):\n",
        "    os.makedirs(dataframe_folder)\n",
        "    \n",
        "split_folder = os.path.join(os.getcwd(), 'splits')\n",
        "splits = {\n",
        "    0: 'train',\n",
        "    1: 'validation',\n",
        "    2: 'test'\n",
        "}\n",
        "dataframe_rows = []\n",
        "\n",
        "debug = True\n",
        "\n",
        "# Create a dataframe for each split\n",
        "for split in splits.values():\n",
        "    folder = os.path.join(split_folder, split)\n",
        "    for filename in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                # open the file\n",
        "                # Store lists of document and labels for each file\n",
        "                document, labels = list(), list()\n",
        "                with open(file_path, mode='r') as text_file:\n",
        "                    for line in text_file:\n",
        "                        splitted = line.split(\"\\t\")\n",
        "                        if len(splitted) > 1:\n",
        "                            word, label = splitted[0:2]\n",
        "                            document.append(word)\n",
        "                            labels.append(label)\n",
        "                            \n",
        "                # create single dataframe row\n",
        "                dataframe_row = {\n",
        "                    \"document\": \" \".join(document),\n",
        "                    \"labels\": \" \".join(labels),\n",
        "                    \"split\": split\n",
        "                }\n",
        "\n",
        "                # print detailed info for the first file\n",
        "                if debug:\n",
        "                    print(\"Information for first file:\")\n",
        "                    print(\"File path:\", file_path)\n",
        "                    print(\"Filename:\", filename)\n",
        "                    print(\"Document:\", document)\n",
        "                    print(\"Labels:\", labels)\n",
        "                    print(\"Split:\", split)\n",
        "                    print(\"Dataframe row:\", dataframe_row)\n",
        "                    debug = False\n",
        "                dataframe_rows.append(dataframe_row)\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "            sys.exit(0)\n",
        "\n",
        "# transform the list of rows in a proper dataframe\n",
        "dataframe = pd.DataFrame(dataframe_rows)\n",
        "dataframe = dataframe[[\"document\",\n",
        "                       \"labels\",\n",
        "                       \"split\"]]\n",
        "dataframe_path = os.path.join(dataframe_folder, \"dataframe.pkl\")\n",
        "dataframe.to_pickle(dataframe_path)\n",
        "print(\"Dataframe successfully saved into:\", dataframe_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Information for first file:\n",
            "File path: /content/splits/train/wsj_0050.dp\n",
            "Filename: wsj_0050.dp\n",
            "Document: ['Cooper', 'Tire', '&', 'Rubber', 'Co.', 'said', 'it', 'has', 'reached', 'an', 'agreement', 'in', 'principle', 'to', 'buy', 'buildings', 'and', 'related', 'property', 'in', 'Albany', ',', 'Ga.', ',', 'from', 'Bridgestone\\\\/Firestone', 'Inc', '.', 'Terms', 'were', \"n't\", 'disclosed', '.', 'The', 'tire', 'maker', 'said', 'the', 'buildings', 'consist', 'of', '1.8', 'million', 'square', 'feet', 'of', 'office', ',', 'manufacturing', 'and', 'warehousing', 'space', 'on', '353', 'acres', 'of', 'land', '.']\n",
            "Labels: ['NNP', 'NNP', 'SYM', 'NNP', 'NNP', 'VBD', 'PRP', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'NN', 'TO', 'VB', 'NNS', 'CC', 'JJ', 'NN', 'IN', 'NNP', ',', 'NNP', ',', 'IN', 'NNP', 'NNP', '.', 'NNS', 'VBD', 'RB', 'VBN', '.', 'DT', 'NN', 'NN', 'VBD', 'DT', 'NNS', 'VBP', 'IN', 'CD', 'CD', 'JJ', 'NNS', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', '.']\n",
            "Split: train\n",
            "Dataframe row: {'document': \"Cooper Tire & Rubber Co. said it has reached an agreement in principle to buy buildings and related property in Albany , Ga. , from Bridgestone\\\\/Firestone Inc . Terms were n't disclosed . The tire maker said the buildings consist of 1.8 million square feet of office , manufacturing and warehousing space on 353 acres of land .\", 'labels': 'NNP NNP SYM NNP NNP VBD PRP VBZ VBN DT NN IN NN TO VB NNS CC JJ NN IN NNP , NNP , IN NNP NNP . NNS VBD RB VBN . DT NN NN VBD DT NNS VBP IN CD CD JJ NNS IN NN , NN CC NN NN IN CD NNS IN NN .', 'split': 'train'}\n",
            "Dataframe successfully saved into: /content/Dataframes/dataframe.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNGE4ASM2ySc"
      },
      "source": [
        "### `DataFrame` loading\n",
        "\n",
        "The following code loads a previously saved `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "elHV28VT2ySc",
        "outputId": "2a41efd4-c900-4226-ea31-2bb465ccb888"
      },
      "source": [
        "dataframe_folder = os.path.join(os.getcwd(), \"Dataframes\")\n",
        "dataframe_path = os.path.join(dataframe_folder, \"dataframe.pkl\")\n",
        "dataframe = pd.read_pickle(dataframe_path)\n",
        "print(\"Full dataframe:\")\n",
        "dataframe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full dataframe:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>labels</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cooper Tire &amp; Rubber Co. said it has reached a...</td>\n",
              "      <td>NNP NNP SYM NNP NNP VBD PRP VBZ VBN DT NN IN N...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>J.P. Bolduc , vice chairman of W.R. Grace &amp; Co...</td>\n",
              "      <td>NNP NNP , NN NN IN NNP NNP CC NNP , WDT VBZ DT...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alleghany Corp. said it completed the acquisit...</td>\n",
              "      <td>NNP NNP VBD PRP VBD DT NN IN NNP NNPS CC NNP N...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The U.S. and Soviet Union are holding technica...</td>\n",
              "      <td>DT NNP CC NNP NNP VBP VBG JJ NNS IN JJ NN IN N...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rekindled hope that two New England states wil...</td>\n",
              "      <td>VBN NN IN CD NNP NNP NNS MD VB JJR JJ NN VBD N...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>Carnival Cruise Lines Inc. said potential prob...</td>\n",
              "      <td>NNP NNP NNP NNP VBD JJ NNS IN DT NN IN CD JJ N...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>Freeport-McMoRan Inc. said it will convert its...</td>\n",
              "      <td>NNP NNP VBD PRP MD VB PRP$ NNP NNP NNPS NNP NN...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Tony Lama Co. said that Equus Investment II Li...</td>\n",
              "      <td>NNP NNP NNP VBD IN NNP NNP NNP NNP NNP VBZ VBN...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Hadson Corp. said it expects to report a third...</td>\n",
              "      <td>NNP NNP VBD PRP VBZ TO VB DT NN JJ NN IN $ CD ...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Two leading constitutional-law experts said Pr...</td>\n",
              "      <td>CD VBG NN NNS VBD NNP NNP VBZ RB VB DT JJ NN T...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>199 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              document  ...  split\n",
              "0    Cooper Tire & Rubber Co. said it has reached a...  ...  train\n",
              "1    J.P. Bolduc , vice chairman of W.R. Grace & Co...  ...  train\n",
              "2    Alleghany Corp. said it completed the acquisit...  ...  train\n",
              "3    The U.S. and Soviet Union are holding technica...  ...  train\n",
              "4    Rekindled hope that two New England states wil...  ...  train\n",
              "..                                                 ...  ...    ...\n",
              "194  Carnival Cruise Lines Inc. said potential prob...  ...   test\n",
              "195  Freeport-McMoRan Inc. said it will convert its...  ...   test\n",
              "196  Tony Lama Co. said that Equus Investment II Li...  ...   test\n",
              "197  Hadson Corp. said it expects to report a third...  ...   test\n",
              "198  Two leading constitutional-law experts said Pr...  ...   test\n",
              "\n",
              "[199 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilxAEdrq2ySc"
      },
      "source": [
        "### Embedding model download\n",
        "\n",
        "Download GloVe embedding model making use of the [Gensim](https://radimrehurek.com/gensim/) library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl72x5nO2ySc",
        "outputId": "c1575a19-fbab-42ca-a7ba-126253dcb034"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_model(embedding_dimension=50):\n",
        "    \n",
        "    download_path = f\"glove-wiki-gigaword-{embedding_dimension}\"\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding dimension! GloVe available embedding dimensions are 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "embedding_dimension = 300\n",
        "\n",
        "embedding_model = load_embedding_model(embedding_dimension)        "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[===============================================---] 95.2% 358.2/376.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zo9Mioz2ySc"
      },
      "source": [
        "## Preliminar steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxqfYjzkiCku"
      },
      "source": [
        "In this section, we define functions to pre-process input data in order to properly feed the neural models.\n",
        "In particular, we will follow these steps:\n",
        "- **Text pre-processing**: tokenize words in order to exploit GloVe embeddings\n",
        "- **Vocabulary creation**: needed to build the co-occurrence matrix and to index and later retrieve words as integers\n",
        "- **OOV terms handling**: assign an embedding vector to words found in the input data, but not in the embedding model\n",
        "- **Embedding matrix computation**: arrange embeddings in a matrix to easily and quickly retrieve them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHKRsCncYOyP"
      },
      "source": [
        "### Text pre-processing\n",
        "\n",
        "Text pre-processing is a critic point when performing POS-tagging: significant alterations of input text could worsen the classification accuracy, particularly at *test time*.\n",
        "\n",
        "Therefore, I decided to only perform **case lowering** (due to the presence of only lower-cased words in the GloVe model) and **brackets substitution** (in the Penn Treebank format, they are encoded as strings - such as `-LRB-` - but the GloVe model features them as they are - in this example `(`) in order to fully exploit *pre-trained* embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP_I6zJZ2ySc",
        "outputId": "ec2c0221-917a-43bd-a548-eacef01af60d"
      },
      "source": [
        "LRB_RE = re.compile('-LRB-')\n",
        "RRB_RE = re.compile('-RRB-')\n",
        "LCB_RE = re.compile('-LCB-')\n",
        "RCB_RE = re.compile('-RCB-')\n",
        "\n",
        "def replace_brackets(text):\n",
        "    # Replaces round and curly brackets tags with actual characters\n",
        "    return LCB_RE.sub('{', RCB_RE.sub('}', LRB_RE.sub('(', RRB_RE.sub(')', text))))\n",
        "\n",
        "def lower(text):\n",
        "    # Transforms given text to lower case.\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          replace_brackets,\n",
        "                          lower\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text, filter_methods=None):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "# Pre-processing\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "print()\n",
        "print('[Debug] Before:\\n{}'.format(dataframe.document[:3]))\n",
        "print()\n",
        "\n",
        "# Replace each sentence with its pre-processed version\n",
        "dataframe['document'] = dataframe['document'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "print('[Debug] After:\\n{}'.format(dataframe.document[:3]))\n",
        "print()\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-processing text...\n",
            "\n",
            "[Debug] Before:\n",
            "0    Cooper Tire & Rubber Co. said it has reached a...\n",
            "1    J.P. Bolduc , vice chairman of W.R. Grace & Co...\n",
            "2    Alleghany Corp. said it completed the acquisit...\n",
            "Name: document, dtype: object\n",
            "\n",
            "[Debug] After:\n",
            "0    cooper tire & rubber co. said it has reached a...\n",
            "1    j.p. bolduc , vice chairman of w.r. grace & co...\n",
            "2    alleghany corp. said it completed the acquisit...\n",
            "Name: document, dtype: object\n",
            "\n",
            "Pre-processing completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdt9eXy2ySc"
      },
      "source": [
        "### Vocabulary creation\n",
        "\n",
        "In order to get GloVe embeddings, we firstly need to create a vocabulary to store unique words from given corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOlbKBQkxHLj"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def build_vocabulary(df):\n",
        "    print(\"Building vocabulary...\")\n",
        "    # Initialize set of unique words\n",
        "    word_listing = set()\n",
        "    ## # Initialize two empty ordered dictionaries\n",
        "    idx_to_word, word_to_idx = OrderedDict(), OrderedDict()\n",
        "    voc_size = 0\n",
        "    for document in tqdm(df, desc=\"Populating set of unique terms...\"):\n",
        "        # Add new words to set\n",
        "        word_listing = word_listing|set(document)\n",
        "        # Add new words to dictionaries\n",
        "        for word in document:\n",
        "            if word not in word_to_idx.keys():\n",
        "                # Fill dictionaries keeping insertion order\n",
        "                voc_size = len(word_to_idx)\n",
        "                idx_to_word[voc_size] = word\n",
        "                word_to_idx[word] = voc_size\n",
        "    print(\"Done!\")\n",
        "    return idx_to_word, word_to_idx, word_listing"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycZ2pnf42ySc"
      },
      "source": [
        "### OOV terms handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOZuVVq82ySc"
      },
      "source": [
        "#### Check number of OOV terms\n",
        "\n",
        "In case the fraction of OOV terms is negligible, we could in principle neglect to handle them.\n",
        "In case of validation/test, we can avoid to consider as OOV words already handled at training-time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2tPf0hU2ySc"
      },
      "source": [
        "def check_OOV_terms(embedding_model, word_listing, training=True, training_oov=None):\n",
        "    \n",
        "    OOV = []\n",
        "    for word in word_listing:\n",
        "        if training is not False:\n",
        "            if word not in embedding_model:\n",
        "                OOV.append(word)\n",
        "        else:\n",
        "            if word not in embedding_model and word not in training_oov:\n",
        "                OOV.append(word)\n",
        "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(OOV), float(len(OOV)) * 100 / len(word_listing)))    \n",
        "    return OOV"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzyoKu7S2ySc"
      },
      "source": [
        "#### Compute co-occurrence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFW_iGq2ySc"
      },
      "source": [
        "Computing a co-occurrence matrix is necessary to provide OOV terms with **non-random embeddings**: we will need to get their *context words* along with their embeddings, in order to compute the OOV embedding vector **averaging** on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJzW4Qqd2ySc"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def sparse_matrix(df, idx_to_word, word_to_idx, window_size):\n",
        "    print(\"Computing sparse matrix...\")\n",
        "\n",
        "    # Sparse matrix inputs\n",
        "    indices = []\n",
        "    data = []\n",
        "    indptr = [0]\n",
        "    \n",
        "    # Dictionary to store co-occurrence as pairs word:{context words:no. of occurrences}\n",
        "    co_dict = OrderedDict()\n",
        "    window_indexes = list(range(-window_size, 0)) + list(range(1, window_size+1))\n",
        "    for document in tqdm(df, desc=\"Populating auxiliary dictionaries...\"):\n",
        "        for i, word in enumerate(document):\n",
        "            # Extract word index from vocabulary\n",
        "            index = word_to_idx[word]\n",
        "            # Define context set extracting words from indexes\n",
        "            context = [document[i+incr] for incr in window_indexes if i+incr in range(len(document))]\n",
        "            # If entry not present, create it\n",
        "            if word not in co_dict:\n",
        "                co_dict[word] = {}\n",
        "            # For every context word, either create the entry or add 1 to it\n",
        "            for context_word in context:\n",
        "                co_dict[word][context_word] = co_dict[word].get(context_word, 0) + 1\n",
        "\n",
        "    \n",
        "    # Loop over ordered dictionary to fill sparse matrix inputs\n",
        "    for word, context in tqdm(co_dict.items(), desc=\"Populating auxiliary arrays...\"):\n",
        "        # List of (vocabulary) indexes of words contained in context\n",
        "        indices += [word_to_idx[key] for key in context.keys()]\n",
        "        # List of slicings for indices and data\n",
        "        indptr.append(len(indices))\n",
        "        # Occurrences of words in context of word\n",
        "        data += list(context.values())\n",
        "    \n",
        "    print(\"Done!\")\n",
        "    # Return co-occurrence matrix\n",
        "    return csr_matrix((data, indices, indptr), dtype=int)\n",
        "\n",
        "def dense_matrix(df, idx_to_word, word_to_idx, window_size):\n",
        "    print(\"Computing dense matrix...\")\n",
        "    # Initialize zeros-filled co-occurrence matrix\n",
        "    com = np.zeros((len(word_to_idx),len(word_to_idx)), dtype=np.int)\n",
        "    for document in tqdm(df, desc=\"Populating matrix...\"):\n",
        "        for i, word in enumerate(document):\n",
        "            # Extract word index from vocabulary\n",
        "            index = word_to_idx[word]\n",
        "            # Define window as list of increments from center word\n",
        "            window_indexes = list(range(-window_size, 0)) + list(range(1, window_size+1))\n",
        "            # Define context extracting words from indexes\n",
        "            context = [document[i+incr] for incr in window_indexes if i+incr in range(len(document))]\n",
        "            # Extract context words index from vocabulary\n",
        "            context_indexes = [word_to_idx[context_word] for context_word in context]\n",
        "            # Update co-occurrence matrix\n",
        "            for context_index in context_indexes:\n",
        "                com[index, context_index] += 1\n",
        "    print(\"Done!\")\n",
        "    return com"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVGzUBE2ySc"
      },
      "source": [
        "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=1):\n",
        "    co_occurrence_matrix = None\n",
        "    sparse = False\n",
        "    \n",
        "    if sparse is not False:\n",
        "        co_occurrence_matrix = sparse_matrix(df, idx_to_word, word_to_idx, window_size)\n",
        "    else:\n",
        "        co_occurrence_matrix = dense_matrix(df, idx_to_word, word_to_idx, window_size)\n",
        "    \n",
        "    print(\"Co-occurrence matrix has shape:\", co_occurrence_matrix.shape)\n",
        "    return co_occurrence_matrix"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkFvXt952ySc"
      },
      "source": [
        "### Embedding matrix computation\n",
        "\n",
        "This matrix collects the **GloVe embeddings** of all the words in the given corpus.\n",
        "\n",
        "Regarding **OOV terms**, the vector is computed as mean of context words' embeddings, when possible. If a OOV term has only other OOV terms as context words, the assigned embedding is a random vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh8cnghV2ySd"
      },
      "source": [
        "# Returns vector embedding (if possible, as average of context words' embeddings)\n",
        "def get_embedding_vector(embedding_model, embedding_dimension, word, contexts):\n",
        "    # Check if word has context (i.e. if any of its neighbours is in the model)\n",
        "    try:\n",
        "        contexts[word]\n",
        "    # If none of its neighbours is in the model, assign random vector\n",
        "    except:\n",
        "        return 1, 0, np.random.normal(size=(embedding_dimension))\n",
        "    # Otherwise, compute the vector as weighted average of the vectors of the words in the context\n",
        "    else:\n",
        "        embeddings = [embedding_model[context_word] for context_word in contexts[word].keys()]\n",
        "        weights = list(contexts[word].values())\n",
        "        return 0, 1, np.average(embeddings, weights=weights, axis=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag1h_J9g2ySd"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, oov_terms, co_occurrence_count_matrix,\n",
        "                           training=True, train_embedding_matrix=None, train_word_to_idx=None):\n",
        "\n",
        "    # Choose how to handle OOV terms: mean or random\n",
        "    random = False\n",
        "    # Indices as in vocabulary\n",
        "    oov_indices = np.array([word_to_idx[term] for term in oov_terms])\n",
        "    # Indices as in oov sparse_matrix\n",
        "    indices, values = co_occurrence_count_matrix[oov_indices,:].nonzero()\n",
        "    # Context dictionary (word:context)\n",
        "    oov_contexts = OrderedDict()\n",
        "    for i, index in enumerate(indices):\n",
        "        # Context contains only words in the model (i.e. having an embedding vector)\n",
        "        context_word = idx_to_word[values[i]]\n",
        "        if context_word in embedding_model:\n",
        "            word = idx_to_word[oov_indices[index]]\n",
        "            count = co_occurrence_count_matrix[oov_indices[index], values[i]]\n",
        "            # {word: {context_words: counts}}\n",
        "            oov_contexts[word] = {**oov_contexts.get(word, {}), **{context_word: count}}\n",
        "   \n",
        "   # Store embeddings in list\n",
        "    vectors = []\n",
        "    n_embedded_vectors = 0\n",
        "    random_vectors = 0\n",
        "    average_vectors = 0\n",
        "    for word in word_to_idx.keys():\n",
        "        vector = None\n",
        "        # If word is in the model, get embedding vector\n",
        "        # If its embedding has already been computed during training, retrieve it\n",
        "        if word not in oov_terms:\n",
        "            if training is not False:\n",
        "                vector = embedding_model[word]\n",
        "            else:\n",
        "                try:\n",
        "                    vector = embedding_model[word]\n",
        "                except KeyError:\n",
        "                    vector = train_embedding_matrix[train_word_to_idx[word]]\n",
        "            n_embedded_vectors += 1\n",
        "        # Otherwise, create a new one (either random or computed from context)\n",
        "        elif random is not False:\n",
        "            vector = np.random.normal(size=(embedding_dimension))\n",
        "            n_oov_vectors += 1\n",
        "        else:\n",
        "            rnd, avg, vector = get_embedding_vector(embedding_model, embedding_dimension, word, oov_contexts)\n",
        "            random_vectors += rnd\n",
        "            average_vectors += avg\n",
        "        \n",
        "        vectors.append(vector)\n",
        "    print(f\"{len(vectors)} vectors in matrix:\\n{n_embedded_vectors} embeddings already in model, {random_vectors+average_vectors} vectors computed for OOV words ({average_vectors} computed as average, {random_vectors} random)\")\n",
        "    # Return concatenation of model embeddings and OOV embeddings\n",
        "    return np.array(vectors)#np.concatenate([vectors, oov_vectors], axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8hAO7ci6pC3"
      },
      "source": [
        "## Models definition\n",
        "\n",
        "Observing documents being of very different lengths, I decided to design the RNN models as to accept **batches of size 1**, i.e. one document at a time, so as to **avoid to perform zero-padding** on input data.\n",
        "Indeed, feeding the model with one sequence at a time, `keras` does not require them to be all of the same length.\n",
        "\n",
        "I took this choice after noticing the significant **slowdown** introduced by **masking** the zero-padded input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCX36wkXfl9G"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abil-SB_ZjfB"
      },
      "source": [
        "We firstly build a simple *Biridectional LSTM (Long Short-Term Memory)*.\n",
        "The output of the model is a **TimeDistributed Dense** (Fully-Connected) layer, in order to collect sequences (timesteps) of words as consecutive inputs, and produce a `n_classes`-dimensional output - representing the one-hot encoded POS tags - for each word in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVXxcedV2ySd"
      },
      "source": [
        "def baseline_model(embedding_dimension, n_classes):\n",
        "    # Model creation\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Bidirectional(layers.LSTM(\n",
        "                                            units=256,\n",
        "                                            return_sequences=True,\n",
        "                                            input_shape=(None, embedding_dimension)\n",
        "                                            ),\n",
        "                                 batch_input_shape=(1, None, embedding_dimension)\n",
        "                                 ),\n",
        "            layers.TimeDistributed(\n",
        "                layers.Dense(n_classes, activation='softmax')\n",
        "                )\n",
        "        ],\n",
        "        name='baseline'\n",
        "    )\n",
        "    # Model description\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUyzZQcLi16A"
      },
      "source": [
        "### GRU model\n",
        "\n",
        "In the second model, a *GRU (Gated Recurrent Unit)* layer substitutes the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpT2hVeajVuS"
      },
      "source": [
        "def gru_model(embedding_dimension, n_classes):\n",
        "    # Model creation\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Bidirectional(layers.GRU(\n",
        "                                            units=256,\n",
        "                                            return_sequences=True,\n",
        "                                            input_shape=(None, embedding_dimension)\n",
        "                                            ),\n",
        "                                 batch_input_shape=(1, None, embedding_dimension)\n",
        "                                 ),\n",
        "            layers.TimeDistributed(\n",
        "                layers.Dense(n_classes, activation='softmax')\n",
        "                )\n",
        "        ],\n",
        "        name='gru'\n",
        "    )\n",
        "    # Model description\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxwSjRUzlz92"
      },
      "source": [
        "### Double LSTM model\n",
        "\n",
        "In the third model, another LSTM layer is added after the first one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kniJNakjl3j4"
      },
      "source": [
        "def double_lstm_model(embedding_dimension, n_classes):\n",
        "    # Model creation\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Bidirectional(layers.LSTM(\n",
        "                                            units=256,\n",
        "                                            return_sequences=True,\n",
        "                                            input_shape=(None, embedding_dimension),\n",
        "                                            ),\n",
        "                                 batch_input_shape=(1, None, embedding_dimension)\n",
        "                                 ),\n",
        "            layers.Bidirectional(layers.LSTM(\n",
        "                                            units=256,\n",
        "                                            return_sequences=True),\n",
        "                                 ),\n",
        "            layers.TimeDistributed(layers.Dense(n_classes, activation='softmax'))\n",
        "        ],\n",
        "        name='double_lstm'\n",
        "    )\n",
        "    # Model description\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByXf60YBARI0"
      },
      "source": [
        "### (Optional) LSTM + CRF Model\n",
        "In the fourth (optional) model, a *CRF (Conditional Random Field)* layer is added after the LSTM.\n",
        "\n",
        "After trying several configurations with both `keras_contrib` and `tf2crf` packages, I gave up trying to build this fourth model, due to the rise of several errors to which I could not quickly figure out solutions.\n",
        "\n",
        "I thus commented the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHnIrRBxV2dr"
      },
      "source": [
        "#!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3OiMFGuAWuv"
      },
      "source": [
        "#   from keras_contrib.layers import CRF\n",
        "#   from keras_contrib.losses import crf_loss\n",
        "#   from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "#   \n",
        "#   def crf_model(embedding_dimension, n_classes):\n",
        "#       # Model creation\n",
        "#       model = keras.Sequential(\n",
        "#           [\n",
        "#               #layers.Masking(mask_value=0., batch_input_shape=(1, None, embedding_dimension)),\n",
        "#               keras.layers.Bidirectional(keras.layers.LSTM(\n",
        "#                                               units=256,\n",
        "#                                               return_sequences=True,\n",
        "#                                               input_shape=(None, embedding_dimension),\n",
        "#                                               ),\n",
        "#                                    batch_input_shape=(1, None, embedding_dimension)\n",
        "#                                    ),\n",
        "#            \n",
        "#               keras.layers.TimeDistributed(keras.layers.Dense(n_classes, activation='softmax'),\n",
        "#               CRF(\n",
        "#                   units=n_classes,\n",
        "#                   learn_mode='join',\n",
        "#                   test_mode='viterbi',\n",
        "#                   sparse_target=False\n",
        "#                   ),   \n",
        "#               #layers.Dense(1, activation=\"sigmoid\")\n",
        "#           ],\n",
        "#           name='crf'\n",
        "#       )\n",
        "#       # Model description\n",
        "#       model.summary()\n",
        "#       return model"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOtzERwNkXJt"
      },
      "source": [
        "### Batch generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgpM0WmDcU2W"
      },
      "source": [
        "A **batch generator** is needed to feed the model with **one** batch at a time, still using the `model.fit()` method. Indeed, it allows us to provide inputs (sequences) of different size (length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7dIUoGqcIdN"
      },
      "source": [
        "def batch_generator(X_data, y_data, start=0):\n",
        "\n",
        "    samples_per_epoch = X_data.shape[0]\n",
        "    number_of_batches = samples_per_epoch # Due to batches being 1-D\n",
        "    counter=start\n",
        "    \n",
        "    while True:\n",
        "        # Reshape as 1-D batch\n",
        "        X_batch = X_data[counter][np.newaxis, ...]\n",
        "        y_batch = y_data[counter][np.newaxis, ...]\n",
        "        counter += 1\n",
        "        yield X_batch,y_batch\n",
        "\n",
        "        # Restart counter to yield data in the next epoch\n",
        "        if counter >= start + number_of_batches:\n",
        "            counter = start"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7TWP_rnulZI"
      },
      "source": [
        "## Prepare training and validation data\n",
        "\n",
        "In this section I firstly load and properly pre-process training and validation data to feed the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GePZM0u6s1Qb"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8VJfCnxvG0"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z41ViuTQ2ySc",
        "outputId": "36569665-b616-4dcc-cf58-26b7f4b7ddc9"
      },
      "source": [
        "# Load training data into dataframe\n",
        "train = dataframe[dataframe['split'] == 'train']\n",
        "# Split into X (document) and y (tags) arrays\n",
        "train_documents = train['document'].apply(lambda txt: txt.split())\n",
        "train_labels = train['labels'].apply(lambda txt: txt.split())\n",
        "print(\"Training documents:\")\n",
        "print(train_documents.head())\n",
        "print(\"Training labels:\")\n",
        "print(train_labels.head())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training documents:\n",
            "0    [cooper, tire, &, rubber, co., said, it, has, ...\n",
            "1    [j.p., bolduc, ,, vice, chairman, of, w.r., gr...\n",
            "2    [alleghany, corp., said, it, completed, the, a...\n",
            "3    [the, u.s., and, soviet, union, are, holding, ...\n",
            "4    [rekindled, hope, that, two, new, england, sta...\n",
            "Name: document, dtype: object\n",
            "Training labels:\n",
            "0    [NNP, NNP, SYM, NNP, NNP, VBD, PRP, VBZ, VBN, ...\n",
            "1    [NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...\n",
            "2    [NNP, NNP, VBD, PRP, VBD, DT, NN, IN, NNP, NNP...\n",
            "3    [DT, NNP, CC, NNP, NNP, VBP, VBG, JJ, NNS, IN,...\n",
            "4    [VBN, NN, IN, CD, NNP, NNP, NNS, MD, VB, JJR, ...\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvROhYCQs5F2"
      },
      "source": [
        "#### Create embedding matrix\n",
        "\n",
        "Create a matrix with shape `(vocabulary_size, embedding_dimension)` to store embeddings for each word in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ksXJDufw8xT",
        "outputId": "78047a07-256f-476f-e855-b2b5b787c81f"
      },
      "source": [
        "# Create vocabulary\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(train_documents)\n",
        "vocabulary_size = len(word_listing)\n",
        "print('Vocabulary size:', vocabulary_size)\n",
        "\n",
        "idx_to_label, label_to_idx, label_listing = build_vocabulary(train_labels)\n",
        "n_classes = len(label_listing)\n",
        "print('Number of classes (POS-tags):', n_classes)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating set of unique terms...: 100%|██████████| 100/100 [00:00<00:00, 4013.53it/s]\n",
            "Populating set of unique terms...: 100%|██████████| 100/100 [00:00<00:00, 11276.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building vocabulary...\n",
            "Done!\n",
            "Vocabulary size: 7404\n",
            "Building vocabulary...\n",
            "Done!\n",
            "Number of classes (POS-tags): 45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSOgQzSvw8sb",
        "outputId": "f8ec7fdd-7489-4a14-a84f-cd0fee561b16"
      },
      "source": [
        "# Check OOV terms\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms: 355 (4.79%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSTC9RxAxAZG",
        "outputId": "e239503b-ec0a-4b9e-cd45-a457404c3779"
      },
      "source": [
        "# Build co-occurrence matrix\n",
        "co_occurrence_matrix = co_occurrence_count(train_documents, idx_to_word, word_to_idx, window_size=1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing dense matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Populating matrix...: 100%|██████████| 100/100 [00:00<00:00, 528.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Co-occurrence matrix has shape: (7404, 7404)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boBk7hxFYnD3",
        "outputId": "478d159b-d4a4-4cfc-c81b-0211b02fefd3"
      },
      "source": [
        "# Build embedding matrix\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, oov_terms, co_occurrence_matrix)\n",
        "\n",
        "print(f\"Training embedding matrix shape: {embedding_matrix.shape}\")\n",
        "print(f\"Training embedding matrix element type: {type(embedding_matrix[0,0])}\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7404 vectors in matrix:\n",
            "7049 embeddings already in model, 355 vectors computed for OOV words (355 computed as average, 0 random)\n",
            "Training embedding matrix shape: (7404, 300)\n",
            "Training embedding matrix element type: <class 'numpy.float64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP9yBhfN2ySd"
      },
      "source": [
        "#### Data pre-processing\n",
        "\n",
        "We need now to vectorize input documents and labels - replacing words with their embedding representations and one-hot encoding tags - in order to feed the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC8FINiUyWgo"
      },
      "source": [
        "# Vectorized documents: sequences of indexes as in word vocabulary\n",
        "X_seq = train_documents.apply(lambda txt: np.array([word_to_idx[i] for i in txt]))\n",
        "# Vectorized labels: sequences of indexes as in label vocabulary\n",
        "y_seq = train_labels.apply(lambda labels: np.array([label_to_idx[i] for i in labels]))\n",
        "\n",
        "# Prepare data to train on batches\n",
        "one_hot_matrix = np.identity((n_classes))\n",
        "# X input data: each word index is replaced with the corresponding embedding\n",
        "X_train_on_batch = X_seq.apply(lambda sequence: embedding_matrix[sequence])\n",
        "# y input data: each label is replaced with the corresponding one-hot encoded label\n",
        "y_train_on_batch = y_seq.apply(lambda sequence: one_hot_matrix[sequence])"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUIzDw2edBCg",
        "outputId": "9356cc2f-a97f-40ff-b5be-c3e9a7b26c8b"
      },
      "source": [
        "# Visualize some examples\n",
        "print(\"X train example:\")\n",
        "print(X_train_on_batch[42])\n",
        "print(\"y train example:\")\n",
        "print(y_train_on_batch[42])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train example:\n",
            "[[ 0.058563    0.093696   -0.16047999 ...  0.11121    -0.48603001\n",
            "   0.42183   ]\n",
            " [-0.40684    -0.33581001  0.46884    ... -0.10412    -0.63138002\n",
            "   0.62768   ]\n",
            " [-0.22994    -0.043043   -0.34722    ... -0.36552    -0.45416999\n",
            "   0.10034   ]\n",
            " ...\n",
            " [ 0.43693     0.085203    0.010649   ...  0.11165    -0.57367998\n",
            "   0.42956001]\n",
            " [ 0.43665001  0.18793    -0.17022    ...  0.032894   -0.52144003\n",
            "   0.22295   ]\n",
            " [-0.12559     0.01363     0.10306    ... -0.34224001 -0.022394\n",
            "   0.13684   ]]\n",
            "y train example:\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a61nZGexMS4"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDhjPFJEx2cG"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNfKJXsBvU4n",
        "outputId": "6319e08a-7c52-4917-8f46-f034f42ac84b"
      },
      "source": [
        "# Load validation data into dataframe\n",
        "val = dataframe[dataframe['split'] == 'validation']\n",
        "# Split into X (document) and y (tags) arrays\n",
        "val_documents = val['document'].apply(lambda txt: txt.split())\n",
        "val_labels = val['labels'].apply(lambda txt: txt.split())\n",
        "print(\"Validation documents:\")\n",
        "print(val_documents.head())\n",
        "print(\"Validation labels:\")\n",
        "print(val_labels.head())"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation documents:\n",
            "100    [allergan, inc., said, it, received, food, and...\n",
            "101    [beauty, takes, backseat, to, safety, on, brid...\n",
            "102    [elco, industries, inc., said, it, expects, ne...\n",
            "103    [american, city, business, journals, inc., sai...\n",
            "104    [the, following, were, among, yesterday, 's, o...\n",
            "Name: document, dtype: object\n",
            "Validation labels:\n",
            "100    [NNP, NNP, VBD, PRP, VBD, NNP, CC, NNP, NNP, N...\n",
            "101    [NN, VBZ, NN, TO, NNP, IN, NNPS, NN, VBZ, IN, ...\n",
            "102    [NNP, NNPS, NNP, VBD, PRP, VBZ, JJ, NN, IN, DT...\n",
            "103    [NNP, NNP, NNP, NNPS, NNP, VBD, PRP$, NN, ,, N...\n",
            "104    [DT, VBG, VBD, IN, NN, POS, NNS, CC, NNS, IN, ...\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBo4F9N7x4iR"
      },
      "source": [
        "#### Create embedding matrix\n",
        "\n",
        "Create a matrix with shape `(vocabulary_size, embedding_dimension)` to store embeddings for each word in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ7jxj9Sx73X",
        "outputId": "866e1804-2944-44b8-df8b-df30c08d31eb"
      },
      "source": [
        "# Create vocabulary\n",
        "val_idx_to_word, val_word_to_idx, val_word_listing = build_vocabulary(val_documents)\n",
        "print('Vocabulary size:', len(val_word_listing))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating set of unique terms...: 100%|██████████| 50/50 [00:00<00:00, 2474.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building vocabulary...\n",
            "Done!\n",
            "Vocabulary size: 5420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVl6Tv6Px78F",
        "outputId": "bf95cbff-331b-452d-9182-6e81bd43719d"
      },
      "source": [
        "# Check OOV terms (making use of training ones)\n",
        "val_oov_terms = check_OOV_terms(embedding_model, val_word_listing, training=False, training_oov=oov_terms)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms: 189 (3.49%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8JIg7HTx76N",
        "outputId": "6af7121c-a0c0-42f6-dc75-fdb01025de5f"
      },
      "source": [
        "# Build co-occurrence matrix\n",
        "val_co_occurrence_matrix = co_occurrence_count(val_documents, val_idx_to_word, val_word_to_idx, window_size=1)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating matrix...: 100%|██████████| 50/50 [00:00<00:00, 388.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing dense matrix...\n",
            "Done!\n",
            "Co-occurrence matrix has shape: (5420, 5420)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZNGZjkVwU28",
        "outputId": "a03555fb-593e-4bdf-8681-86e9f71847fd"
      },
      "source": [
        "# Build embedding matrix\n",
        "val_embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, val_word_to_idx, val_idx_to_word, val_oov_terms, val_co_occurrence_matrix,\n",
        "                                              training=False, train_embedding_matrix=embedding_matrix, train_word_to_idx=word_to_idx)\n",
        "\n",
        "print(f\"Validation embedding matrix shape: {val_embedding_matrix.shape}\")\n",
        "print(f\"Validation embedding matrix element type: {type(val_embedding_matrix[0,0])}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5420 vectors in matrix:\n",
            "5231 embeddings already in model, 189 vectors computed for OOV words (189 computed as average, 0 random)\n",
            "Validation embedding matrix shape: (5420, 300)\n",
            "Validation embedding matrix element type: <class 'numpy.float64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bASTAJM5r-Mw"
      },
      "source": [
        "#### Data pre-processing\n",
        "\n",
        "We need now to vectorize input documents and labels - replacing words with their embedding representations and one-hot encoding tags - in order to feed the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyniLbxu2ySd"
      },
      "source": [
        "# Vectorized documents: replace each word with its embedding\n",
        "X_val_seq = val_documents.apply(lambda txt: np.array([val_word_to_idx[i] for i in txt]))\n",
        "# Vectorized labels: sequences of indexes as in label vocabulary\n",
        "y_val_seq = val_labels.apply(lambda labels: np.array([label_to_idx[i] for i in labels]))\n",
        "\n",
        "# Prepare data to evaluate on batches\n",
        "X_val_on_batch = X_val_seq.apply(lambda sequence: val_embedding_matrix[sequence])\n",
        "y_val_on_batch = y_val_seq.apply(lambda sequence: one_hot_matrix[sequence])"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9RDUcdidej-",
        "outputId": "c4c3fe34-6fcc-46e4-edf5-a0b28ebc4fb0"
      },
      "source": [
        "# Show some examples\n",
        "print(\"X val example:\", X_val_on_batch[142])\n",
        "print(\"y val example:\", y_val_on_batch[142])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X val example: [[-0.44398999  0.12817    -0.25246999 ... -0.20043001 -0.082191\n",
            "  -0.06255   ]\n",
            " [ 0.04656     0.21318001 -0.0074364  ...  0.0090611  -0.20988999\n",
            "   0.053913  ]\n",
            " [ 0.36700001 -0.29550001  0.48686999 ...  0.32574999 -0.053274\n",
            "  -0.20083   ]\n",
            " ...\n",
            " [-0.24132     0.12063     0.1919     ... -0.063158   -0.32837\n",
            "   0.15507001]\n",
            " [ 0.11697    -0.29819    -0.15219    ... -0.56623    -0.22992\n",
            "  -0.098088  ]\n",
            " [-0.12559     0.01363     0.10306    ... -0.34224001 -0.022394\n",
            "   0.13684   ]]\n",
            "y val example: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scT7cHKI2ySc"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-OGSsz_279"
      },
      "source": [
        "### Models creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hKQe8FIeX4Q",
        "outputId": "fd0211a2-77ff-4b58-86e6-c9968a5a9a26"
      },
      "source": [
        "baseline = baseline_model(embedding_dimension, n_classes)\n",
        "gru = gru_model(embedding_dimension, n_classes)\n",
        "double_lstm = double_lstm_model(embedding_dimension, n_classes)\n",
        "#crf = crf_model(embedding_dimension, n_classes)\n",
        "models = {\n",
        "          'baseline' : baseline,\n",
        "          'gru' : gru,\n",
        "          'double_lstm' : double_lstm,\n",
        "          #'crf' : crf\n",
        "}"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_16 (Bidirectio (1, None, 512)            1140736   \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, None, 45)          23085     \n",
            "=================================================================\n",
            "Total params: 1,163,821\n",
            "Trainable params: 1,163,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"gru\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_17 (Bidirectio (1, None, 512)            857088    \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, None, 45)          23085     \n",
            "=================================================================\n",
            "Total params: 880,173\n",
            "Trainable params: 880,173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"double_lstm\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_18 (Bidirectio (1, None, 512)            1140736   \n",
            "_________________________________________________________________\n",
            "bidirectional_19 (Bidirectio (1, None, 512)            1574912   \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, None, 45)          23085     \n",
            "=================================================================\n",
            "Total params: 2,738,733\n",
            "Trainable params: 2,738,733\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcx486xYrz6u"
      },
      "source": [
        "### Optimizers, metrics, callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrFZX2xErQB8"
      },
      "source": [
        "Having a look at some papers, I decided to make use of the **Adam optimizer** algorithm, and to employ **Categorical Crossentropy** as loss function.\n",
        "\n",
        "During training, the tracked metrics are (categorical) *accuracy*, *precision* and *recall*.\n",
        "\n",
        "At this step, it is not meaningful to track **(macro-averaged) F1-score** - which will be the final evaluation score - due to its non-linearity. Indeed, `model.fit()` performes metrics computation one batch at a time, and then averages on all the batches, providing a meaningless result when it comes to such a nonlinear metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o79JN9k6ryac"
      },
      "source": [
        "# Optimizer\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "# Number of epochs\n",
        "n_epochs=40\n",
        "# Metrics to track during training\n",
        "metrics = [\n",
        "           'acc', # Alias for keras.metrics.CategoricalAccuracy(), in case of multi-class output\n",
        "           keras.metrics.Precision(),\n",
        "           keras.metrics.Recall(),\n",
        "            ]\n",
        "# Objective function to minimize\n",
        "loss = keras.losses.CategoricalCrossentropy()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf0Djsb-zRR-"
      },
      "source": [
        "Reducing the **learning rate** after a certain number of epochs helps improving model accuracy along with convergence speed: choosing a constant learning rate could lead to either too slow (low learning rate) or too rough (high learning rate) training.\n",
        "\n",
        "However, this should not be done too early, to avoid an excessive slowdown: I decided to start the reducing after 3/4 of the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw9GHpf20eJc"
      },
      "source": [
        "# Callback to decrease learning rate after 2/3 of the epochs\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < n_epochs *3/4:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * np.math.exp(-0.1)\n",
        "\n",
        "cbk = keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68fe2AzIw3kD"
      },
      "source": [
        "### Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkYZi9CEw5B0"
      },
      "source": [
        "# All at once\n",
        "for model in models.values():\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=opt,\n",
        "        metrics=metrics\n",
        "    )"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR_JEPxEgLYy"
      },
      "source": [
        "### Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR2O61dh2F6Q",
        "outputId": "128bf724-61e2-4cb6-9e30-395130d308c2"
      },
      "source": [
        "# All at once\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name} model...\")\n",
        "    model.fit(\n",
        "    batch_generator(X_train_on_batch, y_train_on_batch, start=0),\n",
        "    epochs=n_epochs,\n",
        "    batch_size=1,\n",
        "    steps_per_epoch=X_train_on_batch.shape[0],\n",
        "    callbacks=[cbk],\n",
        "    # The following line makes the training freeze at the end of the first epoch (?)\n",
        "    # I thus decided to perform evaluation only after training\n",
        "    #validation_data=batch_generator(X_val_on_batch, y_val_on_batch, start=100)\n",
        "    )"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training baseline model...\n",
            "Epoch 1/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 1.6670 - acc: 0.5533 - precision_3: 0.9246 - recall_3: 0.3066\n",
            "Epoch 2/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.6419 - acc: 0.7936 - precision_3: 0.9136 - recall_3: 0.6813\n",
            "Epoch 3/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.4414 - acc: 0.8410 - precision_3: 0.9133 - recall_3: 0.7686\n",
            "Epoch 4/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.3418 - acc: 0.8661 - precision_3: 0.9183 - recall_3: 0.8143\n",
            "Epoch 5/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2776 - acc: 0.8813 - precision_3: 0.9235 - recall_3: 0.8423\n",
            "Epoch 6/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2347 - acc: 0.8923 - precision_3: 0.9278 - recall_3: 0.8607\n",
            "Epoch 7/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1898 - acc: 0.9081 - precision_3: 0.9376 - recall_3: 0.8817\n",
            "Epoch 8/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.1544 - acc: 0.9207 - precision_3: 0.9463 - recall_3: 0.8986\n",
            "Epoch 9/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1279 - acc: 0.9325 - precision_3: 0.9534 - recall_3: 0.9124\n",
            "Epoch 10/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.1069 - acc: 0.9424 - precision_3: 0.9604 - recall_3: 0.9250\n",
            "Epoch 11/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0901 - acc: 0.9506 - precision_3: 0.9660 - recall_3: 0.9362\n",
            "Epoch 12/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0773 - acc: 0.9572 - precision_3: 0.9701 - recall_3: 0.9449\n",
            "Epoch 13/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0674 - acc: 0.9619 - precision_3: 0.9735 - recall_3: 0.9508\n",
            "Epoch 14/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0542 - acc: 0.9692 - precision_3: 0.9787 - recall_3: 0.9605\n",
            "Epoch 15/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0440 - acc: 0.9775 - precision_3: 0.9842 - recall_3: 0.9697\n",
            "Epoch 16/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0361 - acc: 0.9817 - precision_3: 0.9870 - recall_3: 0.9760\n",
            "Epoch 17/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0292 - acc: 0.9863 - precision_3: 0.9906 - recall_3: 0.9818\n",
            "Epoch 18/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0241 - acc: 0.9894 - precision_3: 0.9933 - recall_3: 0.9862\n",
            "Epoch 19/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0208 - acc: 0.9920 - precision_3: 0.9949 - recall_3: 0.9886\n",
            "Epoch 20/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0168 - acc: 0.9946 - precision_3: 0.9965 - recall_3: 0.9921\n",
            "Epoch 21/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0146 - acc: 0.9955 - precision_3: 0.9969 - recall_3: 0.9932\n",
            "Epoch 22/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0114 - acc: 0.9973 - precision_3: 0.9982 - recall_3: 0.9963\n",
            "Epoch 23/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0096 - acc: 0.9978 - precision_3: 0.9986 - recall_3: 0.9969\n",
            "Epoch 24/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0069 - acc: 0.9991 - precision_3: 0.9995 - recall_3: 0.9985\n",
            "Epoch 25/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0052 - acc: 0.9995 - precision_3: 0.9997 - recall_3: 0.9993\n",
            "Epoch 26/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0040 - acc: 0.9998 - precision_3: 0.9998 - recall_3: 0.9996\n",
            "Epoch 27/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0033 - acc: 0.9999 - precision_3: 1.0000 - recall_3: 0.9999\n",
            "Epoch 28/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0028 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 29/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0025 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 30/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0022 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 31/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0020 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 32/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0018 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 33/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0016 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 34/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0015 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 35/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0014 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 36/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0013 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 37/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0013 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 38/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0012 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 39/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0012 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Epoch 40/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0011 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
            "Training gru model...\n",
            "Epoch 1/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 1.0487 - acc: 0.7049 - precision_3: 0.9588 - recall_3: 0.7852\n",
            "Epoch 2/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.4140 - acc: 0.8495 - precision_3: 0.9140 - recall_3: 0.7899\n",
            "Epoch 3/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.3116 - acc: 0.8757 - precision_3: 0.9213 - recall_3: 0.8346\n",
            "Epoch 4/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.2549 - acc: 0.8912 - precision_3: 0.9277 - recall_3: 0.8579\n",
            "Epoch 5/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.2155 - acc: 0.9028 - precision_3: 0.9330 - recall_3: 0.8744\n",
            "Epoch 6/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.1855 - acc: 0.9119 - precision_3: 0.9379 - recall_3: 0.8876\n",
            "Epoch 7/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.1616 - acc: 0.9200 - precision_3: 0.9426 - recall_3: 0.8986\n",
            "Epoch 8/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1418 - acc: 0.9276 - precision_3: 0.9480 - recall_3: 0.9086\n",
            "Epoch 9/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.1250 - acc: 0.9347 - precision_3: 0.9530 - recall_3: 0.9173\n",
            "Epoch 10/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1105 - acc: 0.9412 - precision_3: 0.9573 - recall_3: 0.9254\n",
            "Epoch 11/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0979 - acc: 0.9468 - precision_3: 0.9617 - recall_3: 0.9331\n",
            "Epoch 12/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0868 - acc: 0.9521 - precision_3: 0.9655 - recall_3: 0.9402\n",
            "Epoch 13/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0771 - acc: 0.9571 - precision_3: 0.9694 - recall_3: 0.9460\n",
            "Epoch 14/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0685 - acc: 0.9614 - precision_3: 0.9730 - recall_3: 0.9517\n",
            "Epoch 15/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0610 - acc: 0.9654 - precision_3: 0.9758 - recall_3: 0.9565\n",
            "Epoch 16/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0545 - acc: 0.9693 - precision_3: 0.9782 - recall_3: 0.9610\n",
            "Epoch 17/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0487 - acc: 0.9725 - precision_3: 0.9804 - recall_3: 0.9652\n",
            "Epoch 18/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0432 - acc: 0.9760 - precision_3: 0.9828 - recall_3: 0.9694\n",
            "Epoch 19/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0378 - acc: 0.9801 - precision_3: 0.9859 - recall_3: 0.9741\n",
            "Epoch 20/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0331 - acc: 0.9837 - precision_3: 0.9885 - recall_3: 0.9784\n",
            "Epoch 21/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0291 - acc: 0.9860 - precision_3: 0.9904 - recall_3: 0.9815\n",
            "Epoch 22/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0259 - acc: 0.9884 - precision_3: 0.9920 - recall_3: 0.9840\n",
            "Epoch 23/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0242 - acc: 0.9886 - precision_3: 0.9920 - recall_3: 0.9848\n",
            "Epoch 24/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0226 - acc: 0.9891 - precision_3: 0.9920 - recall_3: 0.9857\n",
            "Epoch 25/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0174 - acc: 0.9933 - precision_3: 0.9952 - recall_3: 0.9911\n",
            "Epoch 26/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0146 - acc: 0.9950 - precision_3: 0.9965 - recall_3: 0.9934\n",
            "Epoch 27/40\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.0126 - acc: 0.9959 - precision_3: 0.9974 - recall_3: 0.9947\n",
            "Epoch 28/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0109 - acc: 0.9969 - precision_3: 0.9979 - recall_3: 0.9959\n",
            "Epoch 29/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0094 - acc: 0.9976 - precision_3: 0.9985 - recall_3: 0.9968\n",
            "Epoch 30/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0082 - acc: 0.9982 - precision_3: 0.9989 - recall_3: 0.9974\n",
            "Epoch 31/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0071 - acc: 0.9986 - precision_3: 0.9992 - recall_3: 0.9980\n",
            "Epoch 32/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0064 - acc: 0.9990 - precision_3: 0.9995 - recall_3: 0.9986\n",
            "Epoch 33/40\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.0057 - acc: 0.9993 - precision_3: 0.9995 - recall_3: 0.9989\n",
            "Epoch 34/40\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.0052 - acc: 0.9994 - precision_3: 0.9996 - recall_3: 0.9990\n",
            "Epoch 35/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0047 - acc: 0.9995 - precision_3: 0.9998 - recall_3: 0.9992\n",
            "Epoch 36/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0044 - acc: 0.9996 - precision_3: 0.9999 - recall_3: 0.9994\n",
            "Epoch 37/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0040 - acc: 0.9998 - precision_3: 0.9999 - recall_3: 0.9996\n",
            "Epoch 38/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0038 - acc: 0.9998 - precision_3: 0.9999 - recall_3: 0.9997\n",
            "Epoch 39/40\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.0036 - acc: 0.9999 - precision_3: 0.9999 - recall_3: 0.9997\n",
            "Epoch 40/40\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0034 - acc: 0.9999 - precision_3: 0.9999 - recall_3: 0.9997\n",
            "Training double_lstm model...\n",
            "Epoch 1/40\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 2.2474 - acc: 0.3596 - precision_3: 0.9855 - recall_3: 0.5593\n",
            "Epoch 2/40\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.8867 - acc: 0.7256 - precision_3: 0.9013 - recall_3: 0.5718\n",
            "Epoch 3/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.6104 - acc: 0.7954 - precision_3: 0.9053 - recall_3: 0.6921\n",
            "Epoch 4/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.4908 - acc: 0.8262 - precision_3: 0.9097 - recall_3: 0.7478\n",
            "Epoch 5/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.4159 - acc: 0.8456 - precision_3: 0.9132 - recall_3: 0.7796\n",
            "Epoch 6/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.3625 - acc: 0.8585 - precision_3: 0.9166 - recall_3: 0.8033\n",
            "Epoch 7/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.3205 - acc: 0.8705 - precision_3: 0.9204 - recall_3: 0.8215\n",
            "Epoch 8/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.2862 - acc: 0.8796 - precision_3: 0.9241 - recall_3: 0.8345\n",
            "Epoch 9/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.2580 - acc: 0.8876 - precision_3: 0.9285 - recall_3: 0.8471\n",
            "Epoch 10/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.2341 - acc: 0.8951 - precision_3: 0.9325 - recall_3: 0.8588\n",
            "Epoch 11/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.2135 - acc: 0.9011 - precision_3: 0.9356 - recall_3: 0.8678\n",
            "Epoch 12/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1954 - acc: 0.9073 - precision_3: 0.9391 - recall_3: 0.8769\n",
            "Epoch 13/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1794 - acc: 0.9130 - precision_3: 0.9422 - recall_3: 0.8846\n",
            "Epoch 14/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1651 - acc: 0.9179 - precision_3: 0.9457 - recall_3: 0.8922\n",
            "Epoch 15/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1523 - acc: 0.9224 - precision_3: 0.9484 - recall_3: 0.8990\n",
            "Epoch 16/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1407 - acc: 0.9273 - precision_3: 0.9513 - recall_3: 0.9047\n",
            "Epoch 17/40\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.1303 - acc: 0.9312 - precision_3: 0.9539 - recall_3: 0.9109\n",
            "Epoch 18/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.1213 - acc: 0.9352 - precision_3: 0.9564 - recall_3: 0.9157\n",
            "Epoch 19/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1146 - acc: 0.9367 - precision_3: 0.9578 - recall_3: 0.9185\n",
            "Epoch 20/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1113 - acc: 0.9376 - precision_3: 0.9575 - recall_3: 0.9196\n",
            "Epoch 21/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.1032 - acc: 0.9430 - precision_3: 0.9608 - recall_3: 0.9258\n",
            "Epoch 22/40\n",
            "100/100 [==============================] - 6s 65ms/step - loss: 0.0938 - acc: 0.9482 - precision_3: 0.9642 - recall_3: 0.9324\n",
            "Epoch 23/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0865 - acc: 0.9516 - precision_3: 0.9669 - recall_3: 0.9374\n",
            "Epoch 24/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0805 - acc: 0.9551 - precision_3: 0.9692 - recall_3: 0.9412\n",
            "Epoch 25/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0745 - acc: 0.9586 - precision_3: 0.9715 - recall_3: 0.9460\n",
            "Epoch 26/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0681 - acc: 0.9622 - precision_3: 0.9735 - recall_3: 0.9502\n",
            "Epoch 27/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0625 - acc: 0.9653 - precision_3: 0.9760 - recall_3: 0.9544\n",
            "Epoch 28/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0579 - acc: 0.9683 - precision_3: 0.9778 - recall_3: 0.9576\n",
            "Epoch 29/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0534 - acc: 0.9710 - precision_3: 0.9800 - recall_3: 0.9616\n",
            "Epoch 30/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0490 - acc: 0.9734 - precision_3: 0.9815 - recall_3: 0.9644\n",
            "Epoch 31/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0455 - acc: 0.9759 - precision_3: 0.9833 - recall_3: 0.9672\n",
            "Epoch 32/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0422 - acc: 0.9781 - precision_3: 0.9852 - recall_3: 0.9709\n",
            "Epoch 33/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0405 - acc: 0.9784 - precision_3: 0.9850 - recall_3: 0.9715\n",
            "Epoch 34/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0382 - acc: 0.9804 - precision_3: 0.9861 - recall_3: 0.9741\n",
            "Epoch 35/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0339 - acc: 0.9834 - precision_3: 0.9889 - recall_3: 0.9778\n",
            "Epoch 36/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0313 - acc: 0.9854 - precision_3: 0.9903 - recall_3: 0.9800\n",
            "Epoch 37/40\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 0.0287 - acc: 0.9868 - precision_3: 0.9913 - recall_3: 0.9823\n",
            "Epoch 38/40\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 0.0266 - acc: 0.9884 - precision_3: 0.9925 - recall_3: 0.9840\n",
            "Epoch 39/40\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 0.0248 - acc: 0.9894 - precision_3: 0.9932 - recall_3: 0.9855\n",
            "Epoch 40/40\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 0.0235 - acc: 0.9903 - precision_3: 0.9938 - recall_3: 0.9866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_8fj7w2ySd"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgMz76G-tEQS"
      },
      "source": [
        "### Models evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOdJOPgsGSNQ"
      },
      "source": [
        "**Punctuation masking**, i.e. removing labels corresponding to symbols (`[ . , : ( ) '' `` # $ ]`), is needed to appropriately compute *F1-score*. In fact, these labels exactly coincide to corresponding tokens in a 1-1 relation, providing little or no information about the model's performance. Indeed, these tokens could be labelled simply through the usage of a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHp-GI_s4h_b"
      },
      "source": [
        "# Define list of punctuation labels\n",
        "punctuation_labels = [\".\", \",\", \":\", \"-LRB-\", \"-RRB-\", \"''\", \"``\", \"#\", \"$\" ]\n",
        "punctuation_indices = [label_to_idx[label] for label in punctuation_labels]\n",
        "\n",
        "# Mask punctuation from DataFrame rows (with expected columns ['y_true', 'y_pred'])\n",
        "def punctuation_mask(row, labels):\n",
        "    # Create 'True' mask\n",
        "    mask = np.ones(row['y_true'].shape, dtype=bool)\n",
        "    # Set to 'False' elements corresponding to labels \n",
        "    for label in punctuation_indices:\n",
        "        mask = np.logical_and(mask, row['y_true'] != label)\n",
        "    # Return masked inputs as DataFrame row\n",
        "    return pd.Series([row['y_true'][mask], row['y_pred'][mask]], index=['y_true', 'y_pred'])"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT-b1EFUP9IF"
      },
      "source": [
        "The following function provides an evaluation of the given model making use of `scikit-learn`'s function `classification_report`.\n",
        "\n",
        "Using `model.evaluate` is inaccurate due to the reasons discussed in this [section](#scrollTo=Zcx486xYrz6u&line=1&uniqifier=1) (i.e. averaging on batches does not provide meaningful information, when it comes to nonlinear functions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLDVxHKCBvYQ"
      },
      "source": [
        "def evaluate_model(X, y_true, model, embedding_dimension, punctuation_indices):\n",
        "    \n",
        "    # Extract predictions from model\n",
        "    y_pred = X.apply(lambda sequence: model.predict(sequence.reshape(1,sequence.shape[0],embedding_dimension)).argmax(-1).reshape(sequence.shape[0]))\n",
        "    # Build a DataFrame with both true labels and predicted ones\n",
        "    y = pd.concat([y_true, y_pred], axis=1).rename(columns={'labels': 'y_true', 'document': 'y_pred'})\n",
        "    # Filter out labels corresponding to punctuation classes\n",
        "    y = y.apply(lambda row: punctuation_mask(row, punctuation_indices), axis=1)\n",
        "    # Concatenate to get two flat arrays\n",
        "    y_true = np.concatenate(list(y['y_true']))\n",
        "    y_pred = np.concatenate(list(y['y_pred']))\n",
        "    # Return report using scikit-learn's function\n",
        "    return classification_report(y_true, y_pred, output_dict=True)['macro avg']"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acnfib5IDdWS",
        "outputId": "990e0791-0809-4678-ea57-bb238c8e4410"
      },
      "source": [
        "# Reverse dictionary (score:name), to later extract best model from maximum value\n",
        "f1_scores = {}\n",
        "# Print report and store f1-score results\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluating {name} model...\")\n",
        "    report = evaluate_model(X_val_on_batch, y_val_seq, model, embedding_dimension, punctuation_indices)\n",
        "    print(tabulate([report.keys(), report.values()]))\n",
        "    f1_scores[report['f1-score']] = name\n",
        "    print('*********')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating baseline model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------  ------------------  ------------------  -------\n",
            "precision           recall              f1-score            support\n",
            "0.6263771469015803  0.6209131493311185  0.6168224812950991  27418\n",
            "------------------  ------------------  ------------------  -------\n",
            "*********\n",
            "Evaluating gru model...\n",
            "------------------  -----------------  ------------------  -------\n",
            "precision           recall             f1-score            support\n",
            "0.7341656966060555  0.699053072853373  0.7045628516604224  27418\n",
            "------------------  -----------------  ------------------  -------\n",
            "*********\n",
            "Evaluating double_lstm model...\n",
            "------------------  ------------------  ------------------  -------\n",
            "precision           recall              f1-score            support\n",
            "0.6012593096138272  0.5689522500476847  0.5773810303316118  27418\n",
            "------------------  ------------------  ------------------  -------\n",
            "*********\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRm7z8CpbP4R"
      },
      "source": [
        "### Best model extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkxDe6F6TCPn"
      },
      "source": [
        "Extract the best-performing model that will be later tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvV_IPoNTWW6",
        "outputId": "f4a07a1f-4919-409c-f984-2470b1163633"
      },
      "source": [
        "best_score = max(f1_scores)\n",
        "best_model = f1_scores[best_score]\n",
        "print(f\"The best model is the {best_model} model, with an f1-score of {best_score}\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best model is the gru model, with an f1-score of 0.7045628516604224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0cwWPAbRUUo"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd3QPzUeRaUf"
      },
      "source": [
        "Finally, let's test our **best-performing** model on test data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQuw3pa-RqkR"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pByU0lXKRqkR",
        "outputId": "bd5ad79b-eb93-4f91-a8ae-a475ea3c8981"
      },
      "source": [
        "# Load Test data into dataframe\n",
        "test = dataframe[dataframe['split'] == 'test']\n",
        "# Split into X (document) and y (tags) arrays\n",
        "test_documents = test['document'].apply(lambda txt: txt.split())\n",
        "test_labels = test['labels'].apply(lambda txt: txt.split())\n",
        "print(\"Test documents:\")\n",
        "print(test_documents.head())\n",
        "print(\"Test labels:\")\n",
        "print(test_labels.head())"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test documents:\n",
            "150    [trinity, industries, inc., said, it, reached,...\n",
            "151    [rms, international, inc., ,, hasbrouk, height...\n",
            "152    [intelogic, trace, inc., ,, san, antonio, ,, t...\n",
            "153    [dell, computer, corp., said, it, cut, prices,...\n",
            "154    [usx, corp., posted, a, 23, %, drop, in, third...\n",
            "Name: document, dtype: object\n",
            "Test labels:\n",
            "150    [NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...\n",
            "151    [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBG, D...\n",
            "152    [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...\n",
            "153    [NNP, NNP, NNP, VBD, PRP, VBD, NNS, IN, JJ, IN...\n",
            "154    [NNP, NNP, VBD, DT, CD, NN, NN, IN, NN, NN, ,,...\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAr4zneGSTZw"
      },
      "source": [
        "#### Create embedding matrix\n",
        "\n",
        "Create a matrix with shape `(vocabulary_size, embedding_dimension)` to store embeddings for each word in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvXA8YnSOI1",
        "outputId": "603f75e0-cd0a-4364-b3d4-13565e947fd5"
      },
      "source": [
        "# Create vocabulary\n",
        "test_idx_to_word, test_word_to_idx, test_word_listing = build_vocabulary(test_documents)\n",
        "print('Vocabulary size:', len(test_word_listing))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating set of unique terms...: 100%|██████████| 49/49 [00:00<00:00, 5071.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building vocabulary...\n",
            "Done!\n",
            "Vocabulary size: 3407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afUJcOp1SOI2",
        "outputId": "2953005c-8af9-4d04-854b-3ae51389fe64"
      },
      "source": [
        "# Check OOV terms (making use of training ones)\n",
        "test_oov_terms = check_OOV_terms(embedding_model, test_word_listing, training=False, training_oov=oov_terms)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms: 140 (4.11%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0Ap6umbSOI2",
        "outputId": "1e319dc4-7462-4392-f422-7f6e33d017c6"
      },
      "source": [
        "# Build co-occurrence matrix\n",
        "test_co_occurrence_matrix = co_occurrence_count(test_documents, test_idx_to_word, test_word_to_idx, window_size=1)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating matrix...: 100%|██████████| 49/49 [00:00<00:00, 860.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing dense matrix...\n",
            "Done!\n",
            "Co-occurrence matrix has shape: (3407, 3407)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaGMURoJSOI2",
        "outputId": "7affa26d-cd47-458d-ed0c-498f7fe4844f"
      },
      "source": [
        "# Build embedding matrix\n",
        "test_embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, test_word_to_idx, test_idx_to_word, test_oov_terms, test_co_occurrence_matrix,\n",
        "                                              training=False, train_embedding_matrix=embedding_matrix, train_word_to_idx=word_to_idx)\n",
        "\n",
        "print(f\"Test embedding matrix shape: {test_embedding_matrix.shape}\")\n",
        "print(f\"Test embedding matrix element type: {type(test_embedding_matrix[0,0])}\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3407 vectors in matrix:\n",
            "3267 embeddings already in model, 140 vectors computed for OOV words (140 computed as average, 0 random)\n",
            "Test embedding matrix shape: (3407, 300)\n",
            "Test embedding matrix element type: <class 'numpy.float64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRJuEmCOSOI2"
      },
      "source": [
        "### Data pre-processing\n",
        "\n",
        "We need now to vectorize input documents and labels - replacing words with their embedding representations and one-hot encoding tags - in order to feed the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtTKgSp_SOI2"
      },
      "source": [
        "# Vectorized documents: replace each word with its embedding\n",
        "X_test_seq = test_documents.apply(lambda txt: np.array([test_word_to_idx[i] for i in txt]))\n",
        "# Vectorized labels: sequences of indexes as in label vocabulary\n",
        "y_test_seq = test_labels.apply(lambda labels: np.array([label_to_idx[i] for i in labels]))\n",
        "\n",
        "# Prepare data to test on batches\n",
        "X_test_on_batch = X_test_seq.apply(lambda sequence: test_embedding_matrix[sequence])\n",
        "y_test_on_batch = y_test_seq.apply(lambda sequence: one_hot_matrix[sequence])"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q-OSqZ4SOI2",
        "outputId": "dd67c056-cab8-4e94-aaa6-54ba475a53a1"
      },
      "source": [
        "# Show some examples\n",
        "print(\"X test example:\", X_test_on_batch[192])\n",
        "print(\"y test example:\", y_test_on_batch[192])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X test example: [[-0.11495     0.28154999  0.42993999 ... -0.29853001 -0.15554\n",
            "   0.37920001]\n",
            " [-0.20197999  0.38538     0.11671    ...  0.022478   -0.3682\n",
            "   0.34619999]\n",
            " [ 0.14841001 -0.035665    0.24180999 ...  0.0035853   0.2269\n",
            "   0.46671   ]\n",
            " ...\n",
            " [-0.53399003 -0.046982    0.36667001 ... -0.27298999 -0.050886\n",
            "  -0.33184001]\n",
            " [-0.13858999  0.25777999 -0.065696   ... -1.13069999 -0.0065591\n",
            "  -0.11813   ]\n",
            " [-0.12559     0.01363     0.10306    ... -0.34224001 -0.022394\n",
            "   0.13684   ]]\n",
            "y test example: [[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNGP7dRNb7fN"
      },
      "source": [
        "### Test best model\n",
        "\n",
        "We finally compute the **macro-averaged F1-score** on **test data** for our best-performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B__si3PrStLj",
        "outputId": "654a92a1-0f17-483d-fb64-7acd8a074347"
      },
      "source": [
        "# Test best model\n",
        "print(f\"Testing {best_model} model...\")\n",
        "report = evaluate_model(X_test_on_batch, y_test_seq, models[best_model], embedding_dimension, punctuation_labels)\n",
        "print(tabulate([report.keys(), report.values()]))\n",
        "print('*********')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing gru model...\n",
            "------------------  ------------------  ------------------  -------\n",
            "precision           recall              f1-score            support\n",
            "0.8052957431378859  0.8059550217031418  0.8000951759188805  13676\n",
            "------------------  ------------------  ------------------  -------\n",
            "*********\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmYJOyjCJXqb",
        "outputId": "7ddf2b42-b5eb-4f99-c329-660ee1306d94"
      },
      "source": [
        "print(f\"The best model ({best_model}) has an f1-score on test-data of {report['f1-score']}\")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best model (gru) has an f1-score on test-data of 0.8000951759188805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}